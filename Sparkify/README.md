# Project: Sparkify - ETL with Postgres & Python

## Project Details
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. They'd like a Postgres database created with tables designed to optimize queries on song play analysis.  

Skills demonstrated in this project: 
* ETL Pipeline with Python
* Data Modeling with Postgres  
* Star Schema Database creation


## ETL Pipeline

  ETL.py will work as following for processing the data:
- Connect to the sparkify datase; drop and create all the tables.
- Parse out each json file and load all of the files into dataframe.
- Song_data and Log_data will be loaded into the fact and dimension tables.

Files will be excuted in the following orders:

- 1.create_tables.py
- 2.etl.py
- 3.test.ipynb

## Datasets
### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
```
data/song_data/A/B/C/TRABCEI128F424C983.json
data/song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.  

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
data/log_data/2018/11/2018-11-12-events.json
data/log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![image](https://github.com/CyndiMorris/AnalyticsProjects/assets/159286868/c6a04a61-e096-4a87-99fc-281e4c0f0539)  

## Data Modeling - Star Schema Database
The Star Schema consists of:   

<img src="https://github.com/CyndiMorris/AnalyticsProjects/blob/main/Sparkify/assets/FactDimensionTables.png" style="width:650px">     

Why use a relational database: 
- JOINS are required
- Structured data types
- Enables efficient data aggregation   
-  SQL proficiency is adequate for this analysis
-  Data volume is not large enough to require big data solutions 

## How to Run

1. Run ***create_tables.py*** to create the database and tables.
2. Run ***etl.py*** to process for loading, extracting and inserting the data.
3. Run ***test.ipynb*** to confirm the creation of database and columns.

## Software and Packages

Python   
Pandas  
Psycopg2   
Glob   
JSON   
SQL   
Sql_queries     
Jupyter Notebook  


## Files
**`data folder`:** where all needed jsons reside: log_data and song_data.  
**`create_tables.py`:** Python script recreates the database and tables used to store the data.  
**`etl.ipynb`:** Python Jupyter Notebook used to explore the data and test the ETL process.  
**`etl.py`:** Python script reads in the Log and Song data files, processes and inserts data into the database.  
**`sql_queries.py`:** Python script that defines all the SQL statements used in this project.  
**`test.ipynb`:** Python Jupyter Notebook that was used to test that data was loaded properly.  
